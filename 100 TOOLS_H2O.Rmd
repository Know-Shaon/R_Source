---
title: "100 TOOLS H2O"
author: "Kishore"
date: "3/17/2017"
output: pdf_document
---

H2o
Resource 
https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.Rmd
https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/gbm-randomforest/GBM_RandomForest_Example.R

```{r}
train<- read.csv("~/train_63qYitG.csv")
str(train)
```

Strategy :
1. Take as little as 6% 2% and 2% of the original data set and tune the hyper parameters.
2. Once hyperparameters were found, train the model with the entire dataset.
3. This data set will give a test performce of .7063



Data set details

data.frame':	131662 obs. of  14 variables:
 $ Trip_ID                    : Factor w/ 131662 levels "T0005689460",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Trip_Distance              : num  6.77 29.47 41.58 61.56 54.95 ...
 $ Type_of_Cab                : Factor w/ 6 levels "","A","B","C",..: 3 3 1 4 4 6 6 3 4 5 ...
 $ Customer_Since_Months      : int  1 10 10 10 10 10 10 2 3 5 ...
 $ Life_Style_Index           : num  2.43 2.78 NA NA 3.03 ...
 $ Confidence_Life_Style_Index: Factor w/ 4 levels "","A","B","C": 2 3 1 1 3 1 4 3 1 3 ...
 $ Destination_Type           : Factor w/ 14 levels "A","B","C","D",..: 1 1 5 1 1 1 2 1 1 1 ...
 $ Customer_Rating            : num  3.9 3.45 3.5 3.45 3.4 ...
 $ Cancellation_Last_1Month   : int  0 0 2 0 4 1 1 0 0 1 ...
 $ Var1                       : int  40 38 NA NA 51 72 83 103 NA NA ...
 $ Var2                       : int  46 56 56 52 49 63 50 46 58 58 ...
 $ Var3                       : int  60 78 77 74 102 91 75 63 92 83 ...
 $ Gender                     : Factor w/ 2 levels "Female","Male": 1 2 2 2 2 2 2 2 2 2 ...
 $ Surge_Pricing_Type         : int  2 2 2 3 2 3 2 2 2 3 ...
 

```{r}
#look for levels and replace with numbers in train set
train$Type_of_Cab=as.numeric(train$Type_of_Cab)
train$Type_of_Cab[train$Type_of_Cab=="A"]<-"1"
train$Type_of_Cab[train$Type_of_Cab=="B"]<-"2"
train$Type_of_Cab[train$Type_of_Cab=="C"]<-"3"
train$Type_of_Cab[train$Type_of_Cab=="D"]<-"4"
train$Type_of_Cab[train$Type_of_Cab=="E"]<-"5"
train$Type_of_Cab=as.numeric(train$Type_of_Cab)

train$Confidence_Life_Style_Index=as.numeric(train$Confidence_Life_Style_Index)
train$Confidence_Life_Style_Index[train$Confidence_Life_Style_Index=="A"]<-"11"
train$Confidence_Life_Style_Index[train$Confidence_Life_Style_Index=="B"]<-"22"
train$Confidence_Life_Style_Index[train$Confidence_Life_Style_Index=="C"]<-"33"
train$Confidence_Life_Style_Index=as.numeric(train$Confidence_Life_Style_Index)


train$Destination_Type=as.numeric(train$Destination_Type)
train$Destination_Type[train$Destination_Type=="A"]<-"5"
train$Destination_Type[train$Destination_Type=="B"]<-"10"
train$Destination_Type[train$Destination_Type=="C"]<-"15"
train$Destination_Type[train$Destination_Type=="D"]<-"20"
train$Destination_Type[train$Destination_Type=="E"]<-"25"
train$Destination_Type[train$Destination_Type=="F"]<-"30"
train$Destination_Type[train$Destination_Type=="G"]<-"35"
train$Destination_Type[train$Destination_Type=="H"]<-"40"
train$Destination_Type[train$Destination_Type=="I"]<-"45"
train$Destination_Type[train$Destination_Type=="J"]<-"50"
train$Destination_Type[train$Destination_Type=="K"]<-"55"
train$Destination_Type[train$Destination_Type=="L"]<-"60"
train$Destination_Type[train$Destination_Type=="M"]<-"65"
train$Destination_Type[train$Destination_Type=="N"]<-"70"
train$Destination_Type=as.numeric(train$Destination_Type)



train$Gender<-ifelse(train$Gender=="Male",1,0)
train$Surge_Pricing_Type=as.factor(train$Surge_Pricing_Type)
train$Type_of_Cab[is.na(train$Type_of_Cab)]<-"6"
train$Type_of_Cab=as.numeric(train$Type_of_Cab)

```


Best features found
```{r}
predictors <- names(train[,2:12])
response<-c('Surge_Pricing_Type')

```

The below are the variables that has high noice 
names(train[,12:14])
#[1] "Var3"               "Gender"             "Surge_Pricing_Type"



Start H2o
```{r}
library(h2o)
h2o.init(nthreads=-1)
## optional: connect to a running H2O cluster
#h2o.init(ip="mycluster", port=55555) 

```





transfer Data to h2o cluster
```{r}
train_temp <- train
train <- as.h2o(train)
```




#sampling
```{r}
splits <- h2o.splitFrame(
  data = train, 
  ratios = c(0.6,0.2),   ## only need to specify 2 fractions, the 3rd is implied
  destination_frames = c("train.hex", "valid.hex", "test.hex"), seed = 1234
)
train <- splits[[1]]
train$Type_of_Cab =as.numeric(train$Type_of_Cab )
valid <- splits[[2]]
valid$Type_of_Cab =as.numeric(valid$Type_of_Cab )
test  <- splits[[3]]
test$Type_of_Cab =as.numeric(test$Type_of_Cab )
```




```{r}
#---------------------------------------------------------------------
#Establish Baseline performance.
## We only provide the required parameters, everything else is default
gbm <- h2o.gbm(x = predictors, y = response, training_frame = train)

## Show a detailed model summary
gbm

## Get the performance on the validation set
h2o.confusionMatrix(h2o.performance(gbm, newdata = valid)) 

#From now on, everything is generic and directly applies to most datasets. We assume that all feature engineering is done at this stage and focus on model tuning.
#For multi-class problems, you can use h2o.logloss() or h2o.confusionMatrix() instead of h2o.auc() and for regression problems, you can use h2o.deviance() or h2o.mse().

#---------------------------------------------------------------------
## h2o.rbind makes a copy here, so it's better to use splitFrame with `ratios = c(0.8)` instead above
gbm <- h2o.gbm(x = predictors, y = response, training_frame = h2o.rbind(train, valid), nfolds = 4, seed = 0xDECAF)

## Show a detailed summary of the cross validation metrics
## This gives you an idea of the variance between the folds
gbm@model$cross_validation_metrics_summary

## Get the cross-validated , by scoring the combined holdout predictions.
## (Instead of taking the average of the metrics across the folds)
h2o.confusionMatrix(h2o.performance(gbm, newdata=h2o.rbind(train,valid))) 
#----------------------------------------------------------------------

```




```{r}
#----------------------------------------------------------------------
#Model by Giving If I am feeling luckey parms
gbm <- h2o.gbm(
  ## standard model parameters
  x = predictors, 
  y = response, 
  training_frame = train, 
  validation_frame = valid,
  
  ## more trees is better if the learning rate is small enough 
  ## here, use "more than enough" trees - we have early stopping
  ntrees = 50,                                                            
  ## smaller learning rate is better (this is a good value for most datasets, but see below for annealing)
  learn_rate=0.01,                                                         
  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
  stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "logloss", 
  ## sample 80% of rows per tree
  sample_rate = 0.8,                                                       
  ## sample 80% of columns per split
  col_sample_rate = 0.8,                                                   
  ## fix a random number generator seed for reproducibility
  seed = 1234,                                                             
  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
  score_tree_interval = 10                                                 
)

```



```{r}
## Get the performance on the validation set
h2o.confusionMatrix(h2o.performance(gbm, valid = TRUE))
#----------------------------------------------------------------------

```





```{r}
#----------------------------------------------------------------------
## Depth 10 is usually plenty of depth for most datasets, but you never know
hyper_params = list( max_depth = seq(1,10,1) )
#hyper_params = list( max_depth = c(4,6,8,12,16,20) ) ##faster for larger datasets

grid <- h2o.grid(
  ## hyper parameters
  hyper_params = hyper_params,
  ## full Cartesian hyper-parameter search
  search_criteria = list(strategy = "Cartesian"),
  ## which algorithm to run
  algorithm="gbm",
  ## identifier for the grid, to later retrieve it
  grid_id="dep_grid",
  ## standard model parameters
  x = predictors, 
  y = response, 
  training_frame = train, 
  validation_frame = valid,
  ## more trees is better if the learning rate is small enough 
  ## here, use "more than enough" trees - we have early stopping
  ntrees = 50,                                                            
  ## smaller learning rate is better
  ## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
  learn_rate = 0.05,                                                         
  ## learning rate annealing: learning_rate shrinks by 1% after every tree 
  ## (use 1.00 to disable, but then lower the learning_rate)
  learn_rate_annealing = 0.99,                                               
  ## sample 80% of rows per tree
  sample_rate = 0.8,                                                       
  ## sample 80% of columns per split
  col_sample_rate = 0.8, 
  ## fix a random number generator seed for reproducibility
  seed = 1234,                                                             
  
  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
  stopping_rounds = 5,
  stopping_tolerance = 1e-4,
  stopping_metric = "logloss", 
  
  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
  score_tree_interval = 10                                                
)

```



```{r}
# by default, display the grid search results sorted by increasing logloss (since this is a classification task
grid                                                                       

## sort the grid models by decreasing AUC
sortedGrid <- h2o.getGrid("dep_grid", sort_by="logloss", decreasing = FALSE)    
sortedGrid   

## find the range of max_depth for the top 5 models
topDepths = sortedGrid@summary_table$max_depth[1:5]                       
minDepth = min(as.numeric(topDepths))
maxDepth = max(as.numeric(topDepths))
minDepth
maxDepth

```




```{r}

#----------------------------------------------------------------------
#Tuning all the hyperameters
#----------------------------------------------------------------------
hyper_params = list( 
  ## restrict the search to the range of max_depth established above
  max_depth = seq(minDepth,maxDepth,1),                                      
  
  ## search a large space of row sampling rates per tree
  sample_rate = seq(0.2,1,0.01),                                             
  
  ## search a large space of column sampling rates per split
  col_sample_rate = seq(0.2,1,0.01),                                         
  
  ## search a large space of column sampling rates per tree
  col_sample_rate_per_tree = seq(0.2,1,0.01),                                
  
  ## search a large space of how column sampling per split should change as a function of the depth of the split
  col_sample_rate_change_per_level = seq(0.9,1.1,0.01),                      
  
  ## search a large space of the number of min rows in a terminal node
  min_rows = 2^seq(0,log2(nrow(train))-1,1),                                 
  
  ## search a large space of the number of bins for split-finding for continuous and integer columns
  nbins = 2^seq(4,10,1),                                                     
  
  ## search a large space of the number of bins for split-finding for categorical columns
  nbins_cats = 2^seq(4,12,1),                                                
  
  ## search a few minimum required relative error improvement thresholds for a split to happen
  min_split_improvement = c(0,1e-8,1e-6,1e-4),                               
  
  ## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
  histogram_type = c("UniformAdaptive","QuantilesGlobal","RoundRobin")       
)

#---------------------------------------------------------------------------------------------
#---------------------------------------------------------------------------------------------
search_criteria = list(
  ## Random grid search
  strategy = "RandomDiscrete",      
  
  ## limit the runtime to 60 minutes
  max_runtime_secs = 3600,         
  
  ## build no more than 100 models
  max_models = 100,                  
  
  ## random number generator seed to make sampling of parameter combinations reproducible
  seed = 1234,                        
  
  ## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
  stopping_rounds = 5,                
  stopping_metric = "logloss",
  stopping_tolerance = 1e-3
)
```




```{r}
grid <- h2o.grid(
  ## hyper parameters
  hyper_params = hyper_params,
  
  ## hyper-parameter search configuration (see above)
  search_criteria = search_criteria,
  
  ## which algorithm to run
  algorithm = "gbm",
  
  ## identifier for the grid, to later retrieve it
  grid_id = "final_grid", 
  
  ## standard model parameters
  x = predictors, 
  y = response, 
  training_frame = train, 
  validation_frame = valid,
  
  ## more trees is better if the learning rate is small enough
  ## use "more than enough" trees - we have early stopping
  ntrees = 50,                                                            
  
  ## smaller learning rate is better
  ## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
  learn_rate = 0.05,                                                         
  
  ## learning rate annealing: learning_rate shrinks by 1% after every tree 
  ## (use 1.00 to disable, but then lower the learning_rate)
  learn_rate_annealing = 0.99,                                               
  
  ## early stopping based on timeout (no model should take more than 1 hour - modify as needed)
  max_runtime_secs = 3600,                                                 
  
  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
  stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "logloss", 
  
  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
  score_tree_interval = 10,                                                
  
  ## base random number generator seed for each model (automatically gets incremented internally for each model)
  seed = 1234                                                             
)
```



```{r}
sortedGrid <- h2o.getGrid("final_grid", sort_by = "logloss", decreasing = FALSE)    
sortedGrid

#Get each model from the grid and get the performance metrics 
for (i in 1:10) {
  gbm <- h2o.getModel(sortedGrid@model_ids[[i]])
  print(h2o.confusionMatrix(h2o.performance(gbm, valid = TRUE)))
}


#Get the ‘test’ set performance using top performing model in the grid
gbm <- h2o.getModel(sortedGrid@model_ids[[5]])
print(h2o.confusionMatrix(h2o.performance(gbm, newdata = test)))

#Get the top performing model summary
gbm@model$model_summary
```



learning rate tune
```{r}

hyper_params = list( learn_rate = seq(.01,.1,.01),learn_rate_annealing=seq(.05,1,.01) )
grid <- h2o.grid(
  hyper_params = hyper_params,
  col_sample_rate =.91,
  col_sample_rate_change_per_level =1.02,
  col_sample_rate_per_tree =.97,
  histogram_type = "UniformAdaptive",
  max_depth =7,
  min_rows =16,
  min_split_improvement =1e-4,
  nbins =64,
  nbins_cats =2048,
  sample_rate=.26,
  search_criteria = search_criteria,  
  algorithm = "gbm", 
  grid_id = "learn_grid",  
  x = predictors, 
  y = response, 
  training_frame = train, 
  validation_frame = valid,
  ntrees = 50,                                                             
# learn_rate = 0.09,                                                         
# learn_rate_annealing = 0.99,                                                
  max_runtime_secs = 3600,                                                  
  stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "logloss", 
  score_tree_interval = 10,                                                
 seed = 1234                                                             
)
sortedGrid <- h2o.getGrid("learn_grid", sort_by = "logloss", decreasing = FALSE)  

for (i in 1:5) {
  gbm <- h2o.getModel(sortedGrid@model_ids[[i]])
  print(h2o.confusionMatrix(h2o.performance(gbm, valid = TRUE)))
}

gbm <- h2o.getModel(sortedGrid@model_ids[[5]])
print(h2o.confusionMatrix(h2o.performance(gbm, newdata = test)))


```



Final model(once the model parms were found train with the entire data) 
This practice will get rid off pissimistic bias 
```{r}
gbm <- h2o.gbm(
  col_sample_rate =.91,
  col_sample_rate_change_per_level =1.02,
  col_sample_rate_per_tree =.97,
  histogram_type = "UniformAdaptive",
  max_depth =7,
  min_rows =16,
  min_split_improvement =1e-4,
  nbins =64,
  nbins_cats =2048,
  sample_rate=.26,
# algorithm = "gbm", 
# grid_id = "learn_grid",  
  x = predictors, 
  y = response, 
  training_frame = h2o.rbind(train, valid, test), 
#validation_frame = valid,
  ntrees = 50,                                                             
  learn_rate = 0.09,                                                         
  learn_rate_annealing = 0.99,                                                
  max_runtime_secs = 3600,                                                  
  stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "logloss", 
  score_tree_interval = 10,                                                
 seed = 1234                                                             
)
print(h2o.confusionMatrix(h2o.performance(gbm, newdata = test)))

```

